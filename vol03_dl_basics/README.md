# Vol.3：ディープラーニングの超基本

この回では、ディープラーニングの基本概念をざっくり理解します。  
**「どういう仕組みで動いているのか」** を直感的に掴むことが目的です。

---

## Google Colabとのリンク（Vol.3）

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TrSaleMane/deep-learning-from-step-by-step/blob/main/vol03_dl_basics/vol03_notebook.ipynb)

[Open in Colab（新しいタブで開くには右クリック）](https://colab.research.google.com/github/TrSaleMane/deep-learning-from-step-by-step/blob/main/vol03_dl_basics/vol03_notebook.ipynb)

---

## 📌 1. ニューラルネットワークとは？

ニューラルネットワーク（Neural Network）は  
**「入力 → 計算 → 出力」** を行う関数の集合体です。

- 入力（画像のピクセルなど）  
- 重み付きの計算  
- 活性化関数による変換  
- 出力（分類結果）

という流れで情報を処理します。

層が深くなるほど複雑な特徴を学習できるため、  
これを **ディープ（深い）ラーニング** と呼びます。

MNIST の場合：

**28×28 の画像 → ニューラルネット → 0〜9 のどれかを予測**

という構造になります。

---

## 📌 2. パーセプトロンの考え方

ニューラルネットの最小単位が **パーセプトロン** です。

パーセプトロンは次のような計算をしています：

1. 入力 × 重み  
2. 足し合わせる  
3. 閾値を超えたら 1、超えなければ 0  

これは「条件を満たすかどうかを判断するスイッチ」のようなものです。

これを大量に組み合わせることで：

- 線の傾き  
- 曲線の形  
- 数字の特徴  

など、複雑なパターンを学習できるようになります。

---

## 📌 3. 活性化関数（ReLU / Sigmoid / Tanh / Softmax）

パーセプトロンをそのまま積み重ねても複雑なことは学習できません。  
そこで必要なのが **活性化関数** です。

活性化関数には、ニューラルネットワークに **非線形性** を与える役割を持ちます。

代表的なもの：

### ● ReLU（Rectified Linear Unit）
- 0 以下は 0  
- 0 より大きければそのまま  
- 計算が速く、深いネットワークでも学習が安定  

### ● Sigmoid
- 出力を 0〜1 に圧縮  
- 古典的だが勾配が小さくなりやすい  

### ● Tanh
- 出力を -1〜1 に圧縮  
- Sigmoid より中心が 0 に近く扱いやすい  

### ● Softmax
- すべての値が 0〜1 の範囲に収まる
- 合計が 1 になる（＝確率として扱える）
- 大きいスコアほど大きい確率になる

MNIST のような画像分類では：

**中間層：ReLU  
出力層：Softmax**

という組み合わせが一般的です。

---

## 📌 4. 損失関数（Loss Function）

モデルがどれだけ間違えているかを数値化するのが **損失関数** です。

MNIST の分類では  
**CrossEntropyLoss（交差エントロピー損失）** がよく使われます。

- 損失が大きい → 間違いが多い  
- 損失が小さい → 正しく分類できている  

モデルはこの損失を **最小化するように重みを更新** していきます。

---

## 📌 5. 学習と推論の違い

### ● 学習（Training）
- 正解ラベルを使う  
- 損失を計算  
- 逆伝播で重みを更新  
- 何度も繰り返して賢くなる  

### ● 推論（Inference）
- 正解ラベルは使わない  
- 学習済みの重みで予測するだけ  
- 計算量が少なく高速  

この違いを理解しておくと、Vol.4 以降の PyTorch コードが読みやすくなります。

---

## ▶ 次のステップ

Vol.4 では  
**「実習環境の準備（GitHub & Colab）」**  
を行い、実際にコードを動かす準備を整えます。
